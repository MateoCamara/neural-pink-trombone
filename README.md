# README

## Introduction

This repository contains the source code used in a doctoral research focused on **Articulatory Neural Synthesis**. The main objective is to develop generative models informed by biological articulators capable of synthesizing voice, integrating vocal tract information to improve synthesis quality.

## Project Description

Traditional neural voice synthesis relies exclusively on the speaker's acoustic information, without considering the characteristics of the human vocal tract. The central hypothesis of this research is that incorporating vocal tract information will enhance voice synthesis quality.

In this project, we propose an approach that integrates biological information from the vocal tract into the neural network. Pre-trained models are used to incorporate vocal information and extract articulatory data. Identifying this information allows its decoding into a physical model, such as the **Pink Trombone**. This means that the capabilities of current voice synthesis models can be leveraged by replacing their decoder with a physical one, which would be more explainable and modifiable.

## Code Structure

The main file is `main.py`, which acts as a generic runner for VAE (Variational Autoencoder) models.

### Main Functions:

- **load_model**: Loads the specified model from the `models` module based on the configuration.
- **load_experiment**: Loads the experiment class from the `experiments` module.
- **load_dataloader**: Loads the appropriate dataloader according to the specified data type (spectrogram, embedding, etc.).
- **main**: The main function that orchestrates the loading of configurations, models, data, and the training process.

### Dependencies:

- Python 3.x
- PyTorch
- PyTorch Lightning
- TensorBoard
- YAML
- Other standard Python packages (`argparse`, `importlib`, `os`, etc.)

## Usage Instructions

1. **Clone the repository**:

   ```bash
   git clone <REPOSITORY_URL>
   ```

2. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

3. **Prepare the data**:

   - Ensure that the data is organized as expected by the dataloaders.
   - Update the necessary paths and parameters in the YAML configuration file.

4. **Run the main script**:

   ```bash
   python main.py --config ./configs/config_encodec_dynamic_0.yaml
   ```

## Configuration

The script's behavior is controlled via a configuration file in YAML format. This file includes parameters for:

- **model_params**: Specifications of the model to use.
- **exp_params**: Parameters related to the experiment (name, patience for early stopping, etc.).
- **data_params**: Information about the data (path, type, batch size, etc.).
- **logging_params**: Configurations for logging with TensorBoard.
- **trainer_params**: Parameters for the PyTorch Lightning trainer.

## Results and Validation

Two main methodologies have been employed to generate the latent representations:

1. **Variational Autoencoder Trained from Scratch**: A VAE was trained to reconstruct the input signal, using a bottleneck layer conditioned by a subnetwork called the "projector" that decodes the voice synthesizer parameters.

2. **Pre-trained Models (EnCodec and Wav2Vec)**: These models were used to avoid training the encoding process from scratch, focusing on training the projection network for acoustic-articulatory inversion.

The models were validated by predicting six different parameters, evaluated using objective metrics and the subjective equivalent metric of ViSQOL, using sounds generated by both the synthesizer and human voices. The results indicate that the predicted parameters can generate vowel sounds similar to human ones when input into the synthesizer.

## Future Contributions

This approach demonstrates the potential of integrating articulatory information into voice synthesis models. Future implications include applications in the medical field and the development of more explainable and modifiable models.

## Bibliographic Citation

Please cite this work as:

@article{camara2024decoding,
  title={Decoding Vocal Articulations from Acoustic Latent Representations},
  author={C{\'a}mara, Mateo and Marcos, Fernando and Blanco, Jos{\'e} Luis},
  journal={arXiv preprint arXiv:2406.14379},
  year={2024}
}

## License

This project is distributed under the **MIT License**.

## Contact

For questions or comments, please contact Mateo CÃ¡mara.
